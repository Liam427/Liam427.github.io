---
title: " AI Bootcamp 열세번째"
layout: single
use_math: true
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
    - AI Bootcamp
tag:
    - AI Bootcamp
    - Diary
    - Post
    - Codestates
---
<p align="center">
  <img src="/assets/img/post/AIbootcamp.jpg" alt="AI Bootcamp"/>
</p>  

# 열세번째 Diary  
---  

### 1. Flow  

> 1. Vector transformation의 목적과 사용예시를 설명 할 수 있다.
> 2. eigenvector / eigenvalue를 설명 할 수 있다.
> 3. 데이터의 feature 수가 늘어나면 생기는 문제점과 이를 handling 하기 위한 방법을 설명 할 수 있다.
> 4. PCA의 목적과 기본원리를 설명 할 수 있다.  

### 2. Result & Info  

> 1. 고유값(eigenvalue) & 고유벡터(eigenvector)
    - **$Ax=\lambda x$**    
    $A$는 고정된 것이고, 두개의 미지수가 $x$와 $\lambda$(lambda)가 있다. 이 식을 만족시키는 $\vec{x}$들이 고유벡터(eigenvector)가 된다. 곱해지는 모든 $\vec{x}$들은 변화시키지만, 변화하는 와중에 변하지 않는 성분이 존재하기 마련이고, 선형시스템 $A$의 그 변하지 않는 성분, 혹은 속성이 고유값(eigenvalues)과 고유벡터(eigenvectors)라는 형태로 표현된 것이다. 
> 2. PCA할때 분산이 클수록 그 feature가 가지고 있는 정보가 많다. numerical한 방법도 있지만 `scikit-learn`의 PCA라이브러리로 구현가능하다.) $\Rightarrow$ 그 feature가 많은 vector가 eigenvector가 된다.  
> 3. one hot encoding  
    - categorical data $\Rightarrow$ numerical data  
    $봄 = \begin{bmatrix} 1\\\0\\\0\\\0\end{bmatrix}$, $여름 = \begin{bmatrix} 0\\\1\\\0\\\0\end{bmatrix}$, $가을 = \begin{bmatrix} 0\\\0\\\1\\\0\end{bmatrix}$, $겨울 = \begin{bmatrix} 0\\\0\\\0\\\1\end{bmatrix}$  
    
### 3. Etc  

> 벌써 3주차 이다. 내가 제대로 가고있나? 하는 의문점이 들긴 하는데, 그래도 꾸준히 잘 해봐야지 않겠나 싶다.  
> 어차피 시작은 했고 끝이 나야 끝나는 거기 때문에 이번 과제를 하면서 bootcamp시작 이후 처음으로 자괴감이라는걸 느꼈다.  
> 마음이 많이 애리긴 하는데 꿋꿋이 가야 되지 않겠나 생각한다.  
> 쭉 해보니 통계학은 처음부터 쫌 디테일하게 포스팅을 해야겠다 싶고, 선대는 이렇게 만이라도 가끔 포스팅을 하면 될 것같다.  
> 새해되고 많은걸 시작하게 되었는데 어느하나 포기하지 말고 어느하나 놓치지 말고 해보자.

<p align="center">
    <a href="https://codestates.com" target = "_blank">
        <img src="https://i.imgur.com/RDAD11M.png" 
        width="300" height="300"
        alt="codestates"/>
    </a>
</p> 